{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a06d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_lazywhere' from 'scipy._lib._util' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\_lib\\_util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, LSTM, GRU, SimpleRNN, Input, InputLayer\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Ensemble Framework\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmdarima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpm\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmdarima\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Financial Data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pmdarima\\__init__.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Stuff we want at top-level\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_arima, ARIMA, AutoARIMA, StepwiseContext, decompose\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acf, autocorr_plot, c, pacf, plot_acf, plot_pacf, \\\n\u001b[32m     54\u001b[39m     tsdisplay\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pmdarima\\arima\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapprox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pmdarima\\arima\\arima.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde, norm\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_array\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m sm\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _validation \u001b[38;5;28;01mas\u001b[39;00m val\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseARIMA\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\statsmodels\\api.py:76\u001b[39m\n\u001b[32m      1\u001b[39m __all__ = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBayesGaussMI\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBinomialBayesMixedGLM\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__version_info__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, distributions, iolib, regression, robust, tools\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__init__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     79\u001b[39m     version \u001b[38;5;28;01mas\u001b[39;00m __version__, version_tuple \u001b[38;5;28;01mas\u001b[39;00m __version_info__\n\u001b[32m     80\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\statsmodels\\distributions\\__init__.py:7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mempirical_distribution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     ECDF, ECDFDiscrete, monotone_fn_inverter, StepFunction\n\u001b[32m      4\u001b[39m     )\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01medgeworth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpandedNormal\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdiscrete\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     genpoisson_p, zipoisson, zigenpoisson, zinegbin,\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m __all__ = [\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mECDF\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mECDFDiscrete\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mzipoisson\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     22\u001b[39m     ]\n\u001b[32m     24\u001b[39m test = PytestTester()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\statsmodels\\distributions\\discrete.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rv_discrete, poisson, nbinom\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gammaln\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _lazywhere\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenericLikelihoodModel\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenpoisson_p_gen\u001b[39;00m(rv_discrete):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name '_lazywhere' from 'scipy._lib._util' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\_lib\\_util.py)"
     ]
    }
   ],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import pickle\n",
    "import datetime as dt\n",
    "\n",
    "# Data Handling and Numerical Computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import mplfinance as mpf\n",
    "\n",
    "# Machine Learning and Deep Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, SimpleRNN, Input, InputLayer\n",
    "\n",
    "# Financial Data\n",
    "import pandas_datareader as web\n",
    "import yfinance as yf\n",
    "\n",
    "# Note: pmdarima import removed due to compatibility issues\n",
    "# We'll implement a simple ARIMA alternative later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59807d8",
   "metadata": {},
   "source": [
    "## 1) Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0db90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using training period: 2020-01-01 to 2023-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15028\\1532081931.py:42: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(COMPANY,TRAIN_START,TRAIN_END)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to data/CBA.AX_data.csv\n",
      "Data saved to data/CBA.AX_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure the necessary directories exist\n",
    "DATA_DIR = \"data\"  # Directory to save the data\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    \n",
    "# Define constants outside the loop\n",
    "COMPANY = 'CBA.AX'\n",
    "DEFAULT_TRAIN_START = '2020-01-01'\n",
    "DEFAULT_TRAIN_END = '2023-01-01'\n",
    "while True:\n",
    "    try:\n",
    "        # Get training start date\n",
    "        TRAIN_START = input(f\"Enter training start date (YYYY-MM-DD) or press Enter for default '{DEFAULT_TRAIN_START}': \")\n",
    "        if not TRAIN_START.strip():\n",
    "            TRAIN_START = DEFAULT_TRAIN_START\n",
    "        \n",
    "        # Get training end date\n",
    "        TRAIN_END = input(f\"Enter training end date (YYYY-MM-DD) or press Enter for default '{DEFAULT_TRAIN_END}': \")\n",
    "        if not TRAIN_END.strip():\n",
    "            TRAIN_END = DEFAULT_TRAIN_END\n",
    "        \n",
    "        # Validate date format\n",
    "        start_date = dt.datetime.strptime(TRAIN_START, '%Y-%m-%d')\n",
    "        end_date = dt.datetime.strptime(TRAIN_END, '%Y-%m-%d')\n",
    "        \n",
    "        # Validate date range\n",
    "        if start_date >= end_date:\n",
    "            print(\"Error: Training end date must be after training start date.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Using training period: {TRAIN_START} to {TRAIN_END}\")\n",
    "        break  # Exit the loop if everything is valid\n",
    "    \n",
    "    except ValueError:\n",
    "        print(\"Invalid date format. Please use YYYY-MM-DD format.\")\n",
    "        print(f\"Using default values: {DEFAULT_TRAIN_START} to {DEFAULT_TRAIN_END}\")\n",
    "        TRAIN_START = DEFAULT_TRAIN_START\n",
    "        TRAIN_END = DEFAULT_TRAIN_END\n",
    "        break  # Or continue to ask again by removing this line\n",
    "\n",
    "# Get the data for the stock AAPL\n",
    "data = yf.download(COMPANY,TRAIN_START,TRAIN_END)\n",
    "data_filename = f\"{DATA_DIR}/{COMPANY}_data.csv\"\n",
    "\n",
    "\n",
    "# If it does not exist, save the data to the file\n",
    "print(f\"Saving data to {data_filename}\")\n",
    "data.to_csv(data_filename)\n",
    "print(f\"Data saved to {data_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb408f32",
   "metadata": {},
   "source": [
    "## 2) Clean and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4a6579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to data/CBA.AX_cleaned_data.csv\n",
      "Cleaned Data Sample:\n",
      "        Date      Price      Close       High        Low   Volume\n",
      "0 2020-01-02  64.933319  65.209704  64.535007  64.860162  1416232\n",
      "1 2020-01-03  65.282860  65.998203  65.234089  65.819367  1622784\n",
      "2 2020-01-06  64.843910  64.933328  64.404957  64.811400  2129260\n",
      "3 2020-01-07  66.006332  66.006332  65.177192  65.697438  2417468\n",
      "4 2020-01-08  65.762466  66.046975  65.055254  66.022590  1719114\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_filename, skiprows=[1,2])\n",
    "\n",
    "# Preprocess the data\n",
    "# The columns actually contain: Date, Price, Close, High, Low, Volume\n",
    "df.columns = ['Date', 'Price', 'Close', 'High', 'Low', 'Volume']\n",
    "\n",
    "# Convert the 'Date' column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Convert the financial data columns to numeric\n",
    "numeric_columns = ['Price', 'Close', 'High', 'Low', 'Volume']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "# Save new cleaned data to a CSV file\n",
    "cleaned_data_filename = f\"{DATA_DIR}/{COMPANY}_cleaned_data.csv\"\n",
    "df.to_csv(cleaned_data_filename, index=False)\n",
    "print(f\"Cleaned data saved to {cleaned_data_filename}\")\n",
    "\n",
    "# Check few rows of the cleaned data\n",
    "print(\"Cleaned Data Sample:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfcf443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose a data splitting method:\n",
      "1. Sequential split by ratio\n",
      "2. Split by date\n",
      "3. Random split by ratio\n",
      "Data split sequentially with 20.0% for testing.\n",
      "Train data shape: (608, 6)\n",
      "Test data shape: (152, 6)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(cleaned_data_filename)\n",
    "\n",
    "def split_by_ratio_sequential(df, test_size_ratio):\n",
    "    \"\"\"Splits the data sequentially based on a ratio.\"\"\"\n",
    "    split_index = int(len(df) * (1 - test_size_ratio))\n",
    "    train_data = df.iloc[:split_index]\n",
    "    test_data = df.iloc[split_index:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def split_by_date(df, cutoff_date):\n",
    "    \"\"\"Splits the data based on a specific cutoff date.\"\"\"\n",
    "    cutoff_date = pd.to_datetime(cutoff_date)\n",
    "    train_data = df[df['Date'] < cutoff_date]\n",
    "    test_data = df[df['Date'] >= cutoff_date]\n",
    "    return train_data, test_data\n",
    "\n",
    "def split_by_ratio_random(df, test_size_ratio):\n",
    "    \"\"\"Splits the data randomly based on a ratio.\"\"\"\n",
    "    train_data, test_data = train_test_split(df, test_size=test_size_ratio, random_state=42, shuffle=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "while True:\n",
    "    print(\"\\nChoose a data splitting method:\")\n",
    "    print(\"1. Sequential split by ratio\")\n",
    "    print(\"2. Split by date\")\n",
    "    print(\"3. Random split by ratio\")\n",
    "    choice = input(\"Enter your choice (1/2/3): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        ratio = float(input(\"Enter the test size ratio (e.g., 0.2 for 20%): \"))\n",
    "        test_ratio = ratio if 0 < ratio < 1 else 0.2\n",
    "        train_data, test_data = split_by_ratio_sequential(df, test_ratio)\n",
    "        print(f\"Data split sequentially with {test_ratio*100}% for testing.\")\n",
    "        break\n",
    "    elif choice == '2':\n",
    "        cutoff_date = input(\"Enter the cutoff date (YYYY-MM-DD): \")\n",
    "        train_data, test_data = split_by_date(df, cutoff_date)\n",
    "        print(f\"Data split by date with cutoff at {cutoff_date}.\")\n",
    "        break\n",
    "    elif choice == '3':\n",
    "        ratio = float(input(\"Enter the test size ratio (e.g., 0.2 for 20%): \"))\n",
    "        test_ratio = ratio if 0 < ratio < 1 else 0.2\n",
    "        train_data, test_data = split_by_ratio_random(df, test_ratio)\n",
    "        print(f\"Data split randomly with {test_ratio*100}% for testing.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
    "        \n",
    "# Print the shapes of the train and test datasets\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d303be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling feature columns...\n",
      "Scaled column: Close\n",
      "  Original range: [48.0495, 96.5344]\n",
      "  Scaled range: [0.0000, 1.0000]\n",
      "Scaled column: High\n",
      "  Original range: [44.4250, 95.6616]\n",
      "  Scaled range: [0.0000, 1.0000]\n",
      "Scaled column: Low\n",
      "  Original range: [46.5615, 96.2451]\n",
      "  Scaled range: [0.0000, 1.0000]\n",
      "Scaled column: Volume\n",
      "  Original range: [350619.0000, 17021972.0000]\n",
      "  Scaled range: [0.0000, 1.0000]\n",
      "\n",
      "Scalers saved to data/CBA.AX_scalers.pkl\n",
      "\n",
      "Scaling Summary:\n",
      "Number of features scaled: 4\n",
      "Scaled features: ['Close', 'High', 'Low', 'Volume']\n",
      "Train data shape after scaling: (608, 6)\n",
      "Test data shape after scaling: (152, 6)\n",
      "Scalers loaded successfully!\n",
      "\n",
      "Sample of scaled training data:\n",
      "         Date      Price     Close      High       Low    Volume\n",
      "0  2020-01-02  64.933319  0.353929  0.392493  0.368305  0.063919\n",
      "1  2020-01-03  65.282860  0.370192  0.406137  0.387611  0.076308\n",
      "2  2020-01-06  64.843910  0.348229  0.389955  0.367323  0.106688\n",
      "3  2020-01-07  66.006332  0.370360  0.405027  0.385157  0.123976\n",
      "4  2020-01-08  65.762466  0.371198  0.402647  0.391701  0.082087\n",
      "\n",
      "Sample of scaled test data:\n",
      "           Date      Price     Close      High       Low    Volume\n",
      "608  2022-05-27  94.966888  0.974467  0.978272  0.977500  0.066638\n",
      "609  2022-05-30  94.904541  0.975937  0.978620  0.980906  0.092581\n",
      "610  2022-05-31  92.945175  0.966017  0.946983  0.968179  0.230431\n",
      "611  2022-06-01  95.038147  0.969140  0.958108  0.945055  0.101579\n",
      "612  2022-06-02  93.907059  0.951690  0.953937  0.956886  0.071678\n"
     ]
    }
   ],
   "source": [
    "# Scale the feature columns and store scalers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "# Define the feature columns to scale (excluding Date if it's a column)\n",
    "# Using the correct column names from the cleaned data: ['Price', 'Close', 'High', 'Low', 'Volume']\n",
    "feature_columns = ['Close', 'High', 'Low', 'Price', 'Volume']\n",
    "\n",
    "# Dictionary to store scalers for each feature\n",
    "scalers = {}\n",
    "scaled_train_data = train_data.copy()\n",
    "scaled_test_data = test_data.copy()\n",
    "\n",
    "print(\"Scaling feature columns...\")\n",
    "for column in feature_columns:\n",
    "    if column in train_data.columns:\n",
    "        # Create a separate scaler for each feature\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "        # Fit the scaler on training data only\n",
    "        scaled_train_data[column] = scaler.fit_transform(train_data[[column]])\n",
    "        \n",
    "        # Transform test data using the same scaler\n",
    "        scaled_test_data[column] = scaler.transform(test_data[[column]])\n",
    "        \n",
    "        # Store the scaler in the dictionary\n",
    "        scalers[column] = scaler\n",
    "        \n",
    "        print(f\"Scaled column: {column}\")\n",
    "        print(f\"  Original range: [{train_data[column].min():.4f}, {train_data[column].max():.4f}]\")\n",
    "        print(f\"  Scaled range: [{scaled_train_data[column].min():.4f}, {scaled_train_data[column].max():.4f}]\")\n",
    "\n",
    "# Save scalers to file for future use\n",
    "scalers_filename = f\"{DATA_DIR}/{COMPANY}_scalers.pkl\"\n",
    "with open(scalers_filename, 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "print(f\"\\nScalers saved to {scalers_filename}\")\n",
    "\n",
    "# Display scaling summary\n",
    "print(f\"\\nScaling Summary:\")\n",
    "print(f\"Number of features scaled: {len(scalers)}\")\n",
    "print(f\"Scaled features: {list(scalers.keys())}\")\n",
    "print(f\"Train data shape after scaling: {scaled_train_data.shape}\")\n",
    "print(f\"Test data shape after scaling: {scaled_test_data.shape}\")\n",
    "\n",
    "# Function to load scalers from file\n",
    "def load_scalers(filename):\n",
    "    \"\"\"Load scalers from pickle file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Scalers file {filename} not found!\")\n",
    "        return None\n",
    "\n",
    "# Function to inverse transform scaled data\n",
    "def inverse_transform_predictions(scaled_predictions, column_name, scalers_dict):\n",
    "    \"\"\"Inverse transform scaled predictions back to original scale\"\"\"\n",
    "    if column_name in scalers_dict:\n",
    "        scaler = scalers_dict[column_name]\n",
    "        return scaler.inverse_transform(scaled_predictions.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "        print(f\"Scaler for column '{column_name}' not found!\")\n",
    "        return scaled_predictions\n",
    "\n",
    "# Example usage of loading scalers (for future sessions)\n",
    "loaded_scalers = load_scalers(scalers_filename)\n",
    "if loaded_scalers:\n",
    "    print(\"Scalers loaded successfully!\")\n",
    "\n",
    "# Display sample of scaled data\n",
    "print(\"\\nSample of scaled training data:\")\n",
    "print(scaled_train_data.head())\n",
    "\n",
    "print(\"\\nSample of scaled test data:\")\n",
    "print(scaled_test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048589a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of days to look back to base the prediction\n",
    "PREDICTION_DAYS = 60 # Original\n",
    "\n",
    "# Function to create the dataset with lookback\n",
    "def create_dataset(data, look_back=1):\n",
    "    \"\"\"Create dataset with lookback feature\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:(i + look_back), :])\n",
    "        y.append(data[i + look_back, 0])  # Assuming the first column is the target\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074e379",
   "metadata": {},
   "source": [
    "# 4) Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22766a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for prerequisite data...\n",
      "✅ Found 'scaled_train_data' and 'scaled_test_data'.\n",
      "\n",
      "--- Data Shapes for Modeling ---\n",
      "X_train shape: (548, 60, 4)\n",
      "y_train shape: (548,)\n",
      "X_test shape: (92, 60, 4)\n",
      "y_test shape: (92,)\n",
      "---------------------------------\n",
      "\n",
      "✅ Data preparation complete. Ready for model building.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- Configuration ---\n",
    "# These variables should be defined in the earlier parts of your notebook.\n",
    "# We define them here for completeness, but you should adjust them as needed.\n",
    "COMPANY = 'CBA.AX'\n",
    "PREDICTION_DAYS = 60 # Lookback window\n",
    "DATA_DIR = \"data\"\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# --- Prerequisite Check ---\n",
    "# Ensure the necessary scaled data from the previous steps is available.\n",
    "print(\"Checking for prerequisite data...\")\n",
    "try:\n",
    "    # These should be created in your data preprocessing steps\n",
    "    scaled_train_data\n",
    "    scaled_test_data\n",
    "    print(\"✅ Found 'scaled_train_data' and 'scaled_test_data'.\")\n",
    "except NameError:\n",
    "    print(\"❌ Critical error: 'scaled_train_data' or 'scaled_test_data' not found.\")\n",
    "    print(\"Please run the data loading and preprocessing cells before this one.\")\n",
    "    # As a fallback for demonstration, we try to load them from the notebook's CSVs\n",
    "    try:\n",
    "        print(\"Attempting to load data from CSVs as a fallback...\")\n",
    "        train_data = pd.read_csv(f\"{DATA_DIR}/{COMPANY}_cleaned_data.csv\").iloc[:608] # Example split\n",
    "        test_data = pd.read_csv(f\"{DATA_DIR}/{COMPANY}_cleaned_data.csv\").iloc[608:] # Example split\n",
    "        # NOTE: This is a simplified scaling. In your actual notebook, you'd load saved scalers.\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        feature_columns = ['Close', 'High', 'Low', 'Price', 'Volume']\n",
    "        scalers = {col: MinMaxScaler(feature_range=(0,1)) for col in feature_columns}\n",
    "        scaled_train_data = train_data.copy()\n",
    "        scaled_test_data = test_data.copy()\n",
    "        for col in feature_columns:\n",
    "            if col in train_data.columns:\n",
    "                scaled_train_data[col] = scalers[col].fit_transform(train_data[[col]])\n",
    "                scaled_test_data[col] = scalers[col].transform(test_data[[col]])\n",
    "        print(\"✅ Fallback data loaded and scaled.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fallback failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Data Preparation for Models ---\n",
    "# This part of the code prepares the data in the sequence format required by recurrent models.\n",
    "\n",
    "# Correct feature columns based on the cleaning step. \n",
    "# The available numeric columns after cleaning are ['Price', 'Close', 'High', 'Low', 'Volume']\n",
    "# We will predict 'Close' price, and use others as features.\n",
    "# Let's define the columns we will use for training. 'Close' must be the first one.\n",
    "feature_columns = ['Close', 'High', 'Low', 'Price', 'Volume']\n",
    "\n",
    "# Select and convert data to numpy arrays\n",
    "train_values = scaled_train_data[feature_columns].values\n",
    "test_values = scaled_test_data[feature_columns].values\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, look_back):\n",
    "    \"\"\"Creates sequences and corresponding labels for time series forecasting.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i, :]) # Sequence of 'look_back' days\n",
    "        y.append(data[i, 0]) # The 'Close' price of the next day\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create training and testing sequences\n",
    "X_train, y_train = create_sequences(train_values, PREDICTION_DAYS)\n",
    "X_test, y_test = create_sequences(test_values, PREDICTION_DAYS)\n",
    "\n",
    "print(\"\\n--- Data Shapes for Modeling ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Validate input shape for the models\n",
    "if X_train.shape[1] != PREDICTION_DAYS or X_train.shape[2] != len(feature_columns):\n",
    "    raise ValueError(\"Data shape mismatch. Check PREDICTION_DAYS and feature_columns.\")\n",
    "else:\n",
    "    print(\"\\n✅ Data preparation complete. Ready for model building.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dbb5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, layers_config, optimizer='adam', loss='mean_squared_error'):\n",
    "    \"\"\"\n",
    "    Builds a Keras Sequential model based on a flexible layer configuration.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input data (e.g., (PREDICTION_DAYS, num_features)).\n",
    "        layers_config (list): A list of dictionaries, where each dictionary defines a layer.\n",
    "            Example: [\n",
    "                {'type': 'lstm', 'units': 50, 'return_sequences': True, 'name': 'lstm_1'},\n",
    "                {'type': 'dropout', 'rate': 0.2},\n",
    "                {'type': 'dense', 'units': 25, 'activation': 'relu'},\n",
    "            ]\n",
    "        optimizer (str): The optimizer to use for compiling the model.\n",
    "        loss (str): The loss function to use.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuilding model with input shape: {input_shape}\")\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the input layer explicitly\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # Add layers based on the configuration\n",
    "    for i, layer_info in enumerate(layers_config):\n",
    "        layer_type = layer_info['type'].lower()\n",
    "        layer_name = layer_info.get('name', f'{layer_type}_{i+1}')\n",
    "        \n",
    "        if layer_type == 'lstm':\n",
    "            model.add(LSTM(\n",
    "                units=layer_info['units'],\n",
    "                return_sequences=layer_info.get('return_sequences', False),\n",
    "                name=layer_name\n",
    "            ))\n",
    "            print(f\" --> Added LSTM layer '{layer_name}' with {layer_info['units']} units.\")\n",
    "            \n",
    "        elif layer_type == 'gru':\n",
    "            model.add(GRU(\n",
    "                units=layer_info['units'],\n",
    "                return_sequences=layer_info.get('return_sequences', False),\n",
    "                name=layer_name\n",
    "            ))\n",
    "            print(f\" --> Added GRU layer '{layer_name}' with {layer_info['units']} units.\")\n",
    "            \n",
    "        elif layer_type == 'rnn':\n",
    "            model.add(SimpleRNN(\n",
    "                units=layer_info['units'],\n",
    "                return_sequences=layer_info.get('return_sequences', False),\n",
    "                name=layer_name\n",
    "            ))\n",
    "            print(f\" --> Added SimpleRNN layer '{layer_name}' with {layer_info['units']} units.\")\n",
    "            \n",
    "        elif layer_type == 'dense':\n",
    "            model.add(Dense(\n",
    "                units=layer_info['units'],\n",
    "                activation=layer_info.get('activation', 'relu'),\n",
    "                name=layer_name\n",
    "            ))\n",
    "            print(f\" --> Added Dense layer '{layer_name}' with {layer_info['units']} units.\")\n",
    "            \n",
    "        elif layer_type == 'dropout':\n",
    "            model.add(Dropout(\n",
    "                rate=layer_info['rate'],\n",
    "                name=layer_name\n",
    "            ))\n",
    "            print(f\" --> Added Dropout layer '{layer_name}' with rate {layer_info['rate']}.\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "\n",
    "    # Add the final output layer\n",
    "    model.add(Dense(1, name='output_layer'))\n",
    "    print(\" --> Added final Dense output layer with 1 unit.\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])\n",
    "    print(\"\\nModel compilation complete.\")\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd45637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Experiment 1: LSTM Model =====\n",
      "\n",
      "Building model with input shape: (60, 4)\n",
      " --> Added LSTM layer 'lstm_1' with 100 units.\n",
      " --> Added Dropout layer 'dropout_2' with rate 0.2.\n",
      " --> Added LSTM layer 'lstm_2' with 50 units.\n",
      " --> Added Dropout layer 'dropout_4' with rate 0.2.\n",
      " --> Added Dense layer 'dense_1' with 25 units.\n",
      " --> Added final Dense output layer with 1 unit.\n",
      "\n",
      "Model compilation complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m42,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m26\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">73,501</span> (287.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m73,501\u001b[0m (287.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">73,501</span> (287.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m73,501\u001b[0m (287.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model for 50 epochs...\n",
      "Epoch 1/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - loss: 0.2062 - mae: 0.3514 - val_loss: 0.0259 - val_mae: 0.1493\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0118 - mae: 0.0882 - val_loss: 0.0110 - val_mae: 0.0912\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0086 - mae: 0.0685 - val_loss: 0.0033 - val_mae: 0.0488\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0052 - mae: 0.0569 - val_loss: 0.0039 - val_mae: 0.0513\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0068 - mae: 0.0640 - val_loss: 0.0133 - val_mae: 0.1062\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0061 - mae: 0.0579 - val_loss: 0.0028 - val_mae: 0.0443\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0051 - mae: 0.0533 - val_loss: 0.0046 - val_mae: 0.0554\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0051 - mae: 0.0542 - val_loss: 0.0146 - val_mae: 0.1124\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0065 - mae: 0.0606 - val_loss: 0.0037 - val_mae: 0.0502\n",
      "Epoch 10/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0061 - mae: 0.0586 - val_loss: 0.0099 - val_mae: 0.0899\n",
      "Epoch 11/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0055 - mae: 0.0574 - val_loss: 0.0044 - val_mae: 0.0546\n",
      "Epoch 12/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0044 - mae: 0.0497 - val_loss: 0.0083 - val_mae: 0.0808\n",
      "Epoch 13/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0045 - mae: 0.0509 - val_loss: 0.0022 - val_mae: 0.0400\n",
      "Epoch 14/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0054 - mae: 0.0548 - val_loss: 0.0065 - val_mae: 0.0695\n",
      "Epoch 15/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0040 - mae: 0.0504 - val_loss: 0.0062 - val_mae: 0.0679\n",
      "Epoch 16/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0042 - mae: 0.0490 - val_loss: 0.0081 - val_mae: 0.0806\n",
      "Epoch 17/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0039 - mae: 0.0468 - val_loss: 0.0095 - val_mae: 0.0886\n",
      "Epoch 18/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0043 - mae: 0.0504 - val_loss: 0.0080 - val_mae: 0.0794\n",
      "Epoch 19/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0039 - mae: 0.0468 - val_loss: 0.0023 - val_mae: 0.0406\n",
      "Epoch 20/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0043 - mae: 0.0496 - val_loss: 0.0097 - val_mae: 0.0902\n",
      "Epoch 21/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0047 - mae: 0.0515 - val_loss: 0.0046 - val_mae: 0.0569\n",
      "Epoch 22/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0032 - mae: 0.0438 - val_loss: 0.0089 - val_mae: 0.0859\n",
      "Epoch 23/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0034 - mae: 0.0468 - val_loss: 0.0040 - val_mae: 0.0526\n",
      "\n",
      "--- LSTM Model Evaluation ---\n",
      "Test Loss (MSE): 0.002246\n",
      "Test Mean Absolute Error (MAE): 0.039989\n",
      "Best model saved to: models\\CBA.AX_lstm_model.keras\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment 1: Long Short-Term Memory (LSTM) Model ---\n",
    "\n",
    "print(\"===== Starting Experiment 1: LSTM Model =====\")\n",
    "\n",
    "# 1. Define Model Architecture\n",
    "lstm_layers = [\n",
    "    {'type': 'lstm', 'units': 100, 'return_sequences': True, 'name': 'lstm_1'},\n",
    "    {'type': 'dropout', 'rate': 0.2},\n",
    "    {'type': 'lstm', 'units': 50, 'return_sequences': False, 'name': 'lstm_2'},\n",
    "    {'type': 'dropout', 'rate': 0.2},\n",
    "    {'type': 'dense', 'units': 25, 'activation': 'relu', 'name': 'dense_1'}\n",
    "]\n",
    "\n",
    "# 2. Define Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "model_filename = os.path.join(MODEL_DIR, f\"{COMPANY}_lstm_model.keras\")\n",
    "\n",
    "# 3. Build the model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "lstm_model = build_model(input_shape, lstm_layers)\n",
    "\n",
    "# 4. Train the model\n",
    "print(f\"\\nTraining LSTM model for {epochs} epochs...\")\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_filename, save_best_only=True, monitor='val_loss')\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5. Evaluate and Save\n",
    "print(\"\\n--- LSTM Model Evaluation ---\")\n",
    "loss, mae = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss (MSE): {loss:.6f}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"Best model saved to: {model_filename}\")\n",
    "print(\"========================================\\n\")\n",
    "\n",
    "# To load this model later:\n",
    "# loaded_lstm_model = load_model(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53a71775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Experiment 2: GRU Model =====\n",
      "\n",
      "Building model with input shape: (60, 4)\n",
      " --> Added GRU layer 'gru_1' with 100 units.\n",
      " --> Added Dropout layer 'dropout_2' with rate 0.2.\n",
      " --> Added GRU layer 'gru_2' with 50 units.\n",
      " --> Added Dropout layer 'dropout_4' with rate 0.2.\n",
      " --> Added Dense layer 'dense_1' with 25 units.\n",
      " --> Added final Dense output layer with 1 unit.\n",
      "\n",
      "Model compilation complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m31,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m22,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m26\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,901</span> (218.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,901\u001b[0m (218.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,901</span> (218.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,901\u001b[0m (218.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GRU model for 50 epochs...\n",
      "Epoch 1/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 0.0847 - mae: 0.2225 - val_loss: 0.0187 - val_mae: 0.1316\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0111 - mae: 0.0818 - val_loss: 0.0060 - val_mae: 0.0709\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0075 - mae: 0.0664 - val_loss: 0.0060 - val_mae: 0.0715\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0064 - mae: 0.0604 - val_loss: 0.0066 - val_mae: 0.0749\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0050 - mae: 0.0519 - val_loss: 0.0036 - val_mae: 0.0520\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0051 - mae: 0.0543 - val_loss: 0.0084 - val_mae: 0.0866\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0046 - mae: 0.0498 - val_loss: 0.0022 - val_mae: 0.0387\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0038 - mae: 0.0470 - val_loss: 0.0013 - val_mae: 0.0294\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0046 - mae: 0.0526 - val_loss: 0.0065 - val_mae: 0.0744\n",
      "Epoch 10/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0034 - mae: 0.0435 - val_loss: 0.0052 - val_mae: 0.0658\n",
      "Epoch 11/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0047 - mae: 0.0513 - val_loss: 0.0053 - val_mae: 0.0667\n",
      "Epoch 12/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0043 - mae: 0.0487 - val_loss: 0.0016 - val_mae: 0.0319\n",
      "Epoch 13/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0043 - mae: 0.0498 - val_loss: 0.0043 - val_mae: 0.0592\n",
      "Epoch 14/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0037 - mae: 0.0455 - val_loss: 0.0097 - val_mae: 0.0938\n",
      "Epoch 15/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0035 - mae: 0.0448 - val_loss: 0.0021 - val_mae: 0.0392\n",
      "Epoch 16/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0062 - mae: 0.0576 - val_loss: 0.0096 - val_mae: 0.0937\n",
      "Epoch 17/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0035 - mae: 0.0441 - val_loss: 0.0016 - val_mae: 0.0318\n",
      "Epoch 18/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0034 - mae: 0.0441 - val_loss: 0.0028 - val_mae: 0.0454\n",
      "\n",
      "--- GRU Model Evaluation ---\n",
      "Test Loss (MSE): 0.001337\n",
      "Test Mean Absolute Error (MAE): 0.029450\n",
      "Best model saved to: models\\CBA.AX_gru_model.keras\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment 2: Gated Recurrent Unit (GRU) Model ---\n",
    "print(\"===== Starting Experiment 2: GRU Model =====\")\n",
    "\n",
    "# 1. Define Model Architecture\n",
    "gru_layers = [\n",
    "    {'type': 'gru', 'units': 100, 'return_sequences': True, 'name': 'gru_1'},\n",
    "    {'type': 'dropout', 'rate': 0.2},\n",
    "    {'type': 'gru', 'units': 50, 'return_sequences': False, 'name': 'gru_2'},\n",
    "    {'type': 'dropout', 'rate': 0.2},\n",
    "    {'type': 'dense', 'units': 25, 'activation': 'relu', 'name': 'dense_1'}\n",
    "]\n",
    "\n",
    "# 2. Define Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "model_filename = os.path.join(MODEL_DIR, f\"{COMPANY}_gru_model.keras\")\n",
    "\n",
    "# 3. Build the model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "gru_model = build_model(input_shape, gru_layers)\n",
    "\n",
    "# 4. Train the model\n",
    "print(f\"\\nTraining GRU model for {epochs} epochs...\")\n",
    "history_gru = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_filename, save_best_only=True, monitor='val_loss')\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5. Evaluate and Save\n",
    "print(\"\\n--- GRU Model Evaluation ---\")\n",
    "loss, mae = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss (MSE): {loss:.6f}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"Best model saved to: {model_filename}\")\n",
    "print(\"======================================\\n\")\n",
    "\n",
    "# To load this model later:\n",
    "# loaded_gru_model = load_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713235a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Experiment 3: Simple RNN Model =====\n",
      "\n",
      "Building model with input shape: (60, 4)\n",
      " --> Added SimpleRNN layer 'rnn_1' with 100 units.\n",
      " --> Added Dropout layer 'dropout_2' with rate 0.3.\n",
      " --> Added SimpleRNN layer 'rnn_2' with 50 units.\n",
      " --> Added Dropout layer 'dropout_4' with rate 0.3.\n",
      " --> Added Dense layer 'dense_1' with 25 units.\n",
      " --> Added final Dense output layer with 1 unit.\n",
      "\n",
      "Model compilation complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,550</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m10,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m7,550\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m26\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,351</span> (75.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,351\u001b[0m (75.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,351</span> (75.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,351\u001b[0m (75.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Simple RNN model for 75 epochs...\n",
      "Epoch 1/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.2184 - mae: 0.3745 - val_loss: 0.0101 - val_mae: 0.0841\n",
      "Epoch 2/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0566 - mae: 0.1865 - val_loss: 0.0091 - val_mae: 0.0801\n",
      "Epoch 3/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0355 - mae: 0.1492 - val_loss: 0.0038 - val_mae: 0.0544\n",
      "Epoch 4/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0359 - mae: 0.1515 - val_loss: 0.0181 - val_mae: 0.1229\n",
      "Epoch 5/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0306 - mae: 0.1373 - val_loss: 0.0222 - val_mae: 0.1394\n",
      "Epoch 6/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0255 - mae: 0.1246 - val_loss: 0.0117 - val_mae: 0.0954\n",
      "Epoch 7/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0194 - mae: 0.1069 - val_loss: 0.0097 - val_mae: 0.0842\n",
      "Epoch 8/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0178 - mae: 0.1074 - val_loss: 0.0085 - val_mae: 0.0786\n",
      "Epoch 9/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0184 - mae: 0.1047 - val_loss: 0.0043 - val_mae: 0.0551\n",
      "Epoch 10/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0155 - mae: 0.0977 - val_loss: 0.0077 - val_mae: 0.0761\n",
      "Epoch 11/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0128 - mae: 0.0878 - val_loss: 0.0090 - val_mae: 0.0851\n",
      "Epoch 12/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0143 - mae: 0.0954 - val_loss: 0.0095 - val_mae: 0.0859\n",
      "Epoch 13/75\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0135 - mae: 0.0910 - val_loss: 0.0042 - val_mae: 0.0544\n",
      "\n",
      "--- Simple RNN Model Evaluation ---\n",
      "Test Loss (MSE): 0.003800\n",
      "Test Mean Absolute Error (MAE): 0.054378\n",
      "Best model saved to: models\\CBA.AX_rnn_model.keras\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment 3: Simple Recurrent Neural Network (RNN) Model ---\n",
    "print(\"===== Starting Experiment 3: Simple RNN Model =====\")\n",
    "\n",
    "# 1. Define Model Architecture\n",
    "rnn_layers = [\n",
    "    {'type': 'rnn', 'units': 100, 'return_sequences': True, 'name': 'rnn_1'},\n",
    "    {'type': 'dropout', 'rate': 0.3}, # Increased dropout for simpler models\n",
    "    {'type': 'rnn', 'units': 50, 'return_sequences': False, 'name': 'rnn_2'},\n",
    "    {'type': 'dropout', 'rate': 0.3},\n",
    "    {'type': 'dense', 'units': 25, 'activation': 'relu', 'name': 'dense_1'}\n",
    "]\n",
    "\n",
    "# 2. Define Hyperparameters\n",
    "epochs = 75 # Might need more epochs for simple RNN to converge\n",
    "batch_size = 32\n",
    "model_filename = os.path.join(MODEL_DIR, f\"{COMPANY}_rnn_model.keras\")\n",
    "\n",
    "# 3. Build the model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "rnn_model = build_model(input_shape, rnn_layers)\n",
    "\n",
    "# 4. Train the model\n",
    "print(f\"\\nTraining Simple RNN model for {epochs} epochs...\")\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_filename, save_best_only=True, monitor='val_loss')\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 5. Evaluate and Save\n",
    "print(\"\\n--- Simple RNN Model Evaluation ---\")\n",
    "loss, mae = rnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss (MSE): {loss:.6f}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"Best model saved to: {model_filename}\")\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "# To load this model later:\n",
    "# loaded_rnn_model = load_model(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62237651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Experiment 4: Random Forest Model =====\n",
      "\n",
      "Preparing Random Forest training data...\n",
      "Random Forest training data shape: (548, 240)\n",
      "Random Forest test data shape: (92, 240)\n",
      "\n",
      "Training Random Forest model with 100 estimators...\n",
      "\n",
      "--- Random Forest Model Evaluation ---\n",
      "Test Loss (MSE): 0.001205\n",
      "Test Mean Absolute Error (MAE): 0.027103\n",
      "Best model saved to: models\\CBA.AX_rf_model.pkl\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment 4: Random Forest Model ---\n",
    "print(\"===== Starting Experiment 4: Random Forest Model =====\")\n",
    "\n",
    "# 1. Define Model Configuration\n",
    "rf_config = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1  # Use all available cores\n",
    "}\n",
    "\n",
    "# 2. Define Hyperparameters\n",
    "model_filename = os.path.join(MODEL_DIR, f\"{COMPANY}_rf_model.pkl\")\n",
    "\n",
    "# 3. Prepare data for Random Forest (flatten sequences to 2D)\n",
    "def prepare_rf_data(data, look_back):\n",
    "    \"\"\"Prepare data for Random Forest by creating features from the last 'look_back' days.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i, :])  # Last 'look_back' days as features\n",
    "        y.append(data[i, 0])  # The 'Close' price of the next day\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(\"\\nPreparing Random Forest training data...\")\n",
    "X_train_rf, y_train_rf = prepare_rf_data(train_values, PREDICTION_DAYS)\n",
    "X_test_rf, y_test_rf = prepare_rf_data(test_values, PREDICTION_DAYS)\n",
    "\n",
    "# Reshape to 2D for Random Forest (flatten sequences)\n",
    "X_train_rf = X_train_rf.reshape(X_train_rf.shape[0], -1)\n",
    "X_test_rf = X_test_rf.reshape(X_test_rf.shape[0], -1)\n",
    "\n",
    "print(f\"Random Forest training data shape: {X_train_rf.shape}\")\n",
    "print(f\"Random Forest test data shape: {X_test_rf.shape}\")\n",
    "\n",
    "# 4. Build and train the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(**rf_config)\n",
    "\n",
    "print(f\"\\nTraining Random Forest model with {rf_config['n_estimators']} estimators...\")\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# 5. Evaluate and Save\n",
    "print(\"\\n--- Random Forest Model Evaluation ---\")\n",
    "rf_predictions = rf_model.predict(X_test_rf)\n",
    "\n",
    "# Calculate metrics (matching the neural network evaluation format)\n",
    "rf_mse = mean_squared_error(y_test_rf, rf_predictions)\n",
    "rf_mae = mean_absolute_error(y_test_rf, rf_predictions)\n",
    "\n",
    "print(f\"Test Loss (MSE): {rf_mse:.6f}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {rf_mae:.6f}\")\n",
    "\n",
    "# Save the model\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(f\"Best model saved to: {model_filename}\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# To load this model later:\n",
    "# with open(model_filename, 'rb') as f:\n",
    "#     loaded_rf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bfeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment 5: Simple Moving Average Model (ARIMA Alternative) ---\n",
    "print(\"===== Starting Experiment 5: Simple Moving Average Model =====\")\n",
    "\n",
    "# Due to pmdarima compatibility issues, we'll use a simple moving average as an alternative\n",
    "# This provides a baseline time series forecast\n",
    "\n",
    "# 1. Define Model Configuration\n",
    "ma_window = 30  # 30-day moving average\n",
    "\n",
    "# 2. Define Hyperparameters\n",
    "model_filename = os.path.join(MODEL_DIR, f\"{COMPANY}_ma_model.pkl\")\n",
    "\n",
    "# 3. Prepare data for Moving Average (use original unscaled data)\n",
    "print(\"\\nPreparing Moving Average training data...\")\n",
    "train_data_ma = train_data['Close']\n",
    "test_data_ma = test_data['Close']\n",
    "\n",
    "print(f\"MA training data shape: {train_data_ma.shape}\")\n",
    "print(f\"MA test data shape: {test_data_ma.shape}\")\n",
    "\n",
    "# 4. Create Moving Average Model\n",
    "class SimpleMovingAverageModel:\n",
    "    def __init__(self, window=30):\n",
    "        self.window = window\n",
    "        self.train_data = None\n",
    "    \n",
    "    def fit(self, train_data):\n",
    "        \"\"\"Fit the model with training data\"\"\"\n",
    "        self.train_data = train_data\n",
    "        return self\n",
    "    \n",
    "    def predict(self, n_periods):\n",
    "        \"\"\"Generate predictions for n_periods ahead\"\"\"\n",
    "        if self.train_data is None:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        # Use the last 'window' values to predict\n",
    "        last_values = self.train_data.tail(self.window)\n",
    "        prediction = last_values.mean()\n",
    "        \n",
    "        # For simplicity, return the same prediction for all periods\n",
    "        # In a real scenario, you'd use a more sophisticated approach\n",
    "        predictions = np.full(n_periods, prediction)\n",
    "        return predictions\n",
    "\n",
    "# 5. Build and fit the model\n",
    "print(f\"\\nFitting Moving Average model with window size {ma_window}...\")\n",
    "ma_model = SimpleMovingAverageModel(window=ma_window)\n",
    "ma_model.fit(train_data_ma)\n",
    "\n",
    "print(\"\\nMoving Average Model Summary:\")\n",
    "print(f\"Window size: {ma_window}\")\n",
    "print(f\"Training data size: {len(train_data_ma)}\")\n",
    "\n",
    "# 6. Evaluate and Save\n",
    "print(\"\\n--- Moving Average Model Evaluation ---\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "n_periods = len(test_data_ma)\n",
    "ma_predictions = ma_model.predict(n_periods)\n",
    "\n",
    "# For a more realistic approach, let's implement a rolling prediction\n",
    "ma_predictions_rolling = []\n",
    "combined_data = pd.concat([train_data_ma, test_data_ma])\n",
    "\n",
    "for i in range(len(train_data_ma), len(combined_data)):\n",
    "    # Use the last 'window' values to predict the next value\n",
    "    window_data = combined_data.iloc[i-ma_window:i]\n",
    "    prediction = window_data.mean()\n",
    "    ma_predictions_rolling.append(prediction)\n",
    "\n",
    "ma_predictions = np.array(ma_predictions_rolling)\n",
    "\n",
    "# Calculate metrics (matching the other model evaluation format)\n",
    "ma_mse = mean_squared_error(test_data_ma, ma_predictions)\n",
    "ma_mae = mean_absolute_error(test_data_ma, ma_predictions)\n",
    "\n",
    "print(f\"Test Loss (MSE): {ma_mse:.6f}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {ma_mae:.6f}\")\n",
    "\n",
    "# Save the model\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(ma_model, f)\n",
    "print(f\"Best model saved to: {model_filename}\")\n",
    "\n",
    "print(f\"\\nMoving Average Predictions (first 10): {ma_predictions[:10]}\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Store predictions for ensemble use\n",
    "arima_predictions = ma_predictions  # Alias for compatibility with ensemble code\n",
    "\n",
    "# To load this model later:\n",
    "# with open(model_filename, 'rb') as f:\n",
    "#     loaded_ma_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe380498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "Descaled Predictions (first 10):\n",
      "LSTM: [90.412056 90.49574  90.49525  90.42146  90.3138   90.21818  90.04741\n",
      " 89.8641   89.6634   89.42093 ]\n",
      "GRU: [90.4396   90.477325 90.227356 89.87429  89.52239  89.36612  88.99827\n",
      " 88.71105  88.4825   88.156395]\n",
      "RNN: [91.138695 91.18631  90.38891  90.19528  90.4472   89.8353   89.563866\n",
      " 89.51776  89.31255  89.45607 ]\n",
      "Random Forest: [91.67029083 91.55530221 89.75005274 89.54228794 89.51510316 89.94354429\n",
      " 88.13143792 88.76574575 88.89943207 87.74289607]\n"
     ]
    }
   ],
   "source": [
    "# Descale predictions for comparison for all models\n",
    "def descale_predictions(predictions, column_name, scalers_dict):\n",
    "    \"\"\"Descale predictions using the stored scalers.\"\"\"\n",
    "    if column_name in scalers_dict:\n",
    "        scaler = scalers_dict[column_name]\n",
    "        return scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "        print(f\"Scaler for column '{column_name}' not found!\")\n",
    "        return predictions\n",
    "\n",
    "# Check if models exist before making predictions\n",
    "try:\n",
    "    # Descale predictions for each model\n",
    "    print(\"Generating and descaling predictions...\")\n",
    "    \n",
    "    # Neural network predictions (need descaling)\n",
    "    descaled_lstm_predictions = inverse_transform_predictions(lstm_model.predict(X_test), 'Close', scalers)\n",
    "    descaled_gru_predictions = inverse_transform_predictions(gru_model.predict(X_test), 'Close', scalers)\n",
    "    descaled_rnn_predictions = inverse_transform_predictions(rnn_model.predict(X_test), 'Close', scalers)\n",
    "    descaled_rf_predictions = inverse_transform_predictions(rf_model.predict(X_test_rf), 'Close', scalers)\n",
    "    \n",
    "    # Moving average predictions (already in original scale, but stored as arima_predictions)\n",
    "    descaled_arima_predictions = arima_predictions  # MA predictions are already in original scale\n",
    "    \n",
    "    # Display descaled predictions\n",
    "    print(\"\\nDescaled Predictions (first 10):\")\n",
    "    print(f\"LSTM: {descaled_lstm_predictions[:10]}\")\n",
    "    print(f\"GRU: {descaled_gru_predictions[:10]}\")\n",
    "    print(f\"RNN: {descaled_rnn_predictions[:10]}\")\n",
    "    print(f\"Random Forest: {descaled_rf_predictions[:10]}\")\n",
    "    print(f\"Moving Average (ARIMA alternative): {descaled_arima_predictions[:10]}\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"❌ Error: Some models or variables are not defined: {e}\")\n",
    "    print(\"Please ensure all model training cells have been executed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a1761dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arima_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensemble Modeling (Requirement 1)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Create an ensemble of LSTM + ARIMA model.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m min_len = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(descaled_lstm_predictions), \u001b[38;5;28mlen\u001b[39m(\u001b[43marima_predictions\u001b[49m), \u001b[38;5;28mlen\u001b[39m(rf_predictions))\n\u001b[32m      4\u001b[39m aligned_lstm_preds = descaled_lstm_predictions[-min_len:]\n\u001b[32m      5\u001b[39m aligned_arima_preds = arima_predictions[-min_len:]\n",
      "\u001b[31mNameError\u001b[39m: name 'arima_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensemble Modeling (Requirement 1)\n",
    "# Create an ensemble of LSTM + ARIMA model.\n",
    "min_len = min(len(descaled_lstm_predictions), len(arima_predictions), len(rf_predictions))\n",
    "aligned_lstm_preds = descaled_lstm_predictions[-min_len:]\n",
    "aligned_arima_preds = arima_predictions[-min_len:]\n",
    "aligned_rf_preds = rf_predictions[-min_len:]\n",
    "aligned_y_test = y_test[-min_len:] # The actual values for comparison\n",
    "aligned_gru_preds = descaled_gru_predictions[-min_len:]\n",
    "\n",
    "\n",
    "# --- Ensemble 1: ARIMA + LSTM ---\n",
    "ensemble_arima_lstm = (aligned_arima_preds + aligned_lstm_preds) / 2.0\n",
    "\n",
    "# --- Ensemble 2: ARIMA + LSTM + GRU ---\n",
    "ensemble_arima_lstm_gru = (aligned_arima_preds + aligned_lstm_preds + aligned_gru_preds) / 3.0\n",
    "\n",
    "# --- Ensemble 3: ARIMA + LSTM + GRU + Random Forest ---\n",
    "ensemble_arima_lstm_gru_rf = (aligned_arima_preds + aligned_lstm_preds + aligned_gru_preds + aligned_rf_preds) / 4.0\n",
    "\n",
    "# Calculate MAE for each ensemble\n",
    "print(\"\\n--- Ensemble Model Evaluation ---\")\n",
    "ensemble_mae_arima_lstm = mean_absolute_error(aligned_y_test, ensemble_arima_lstm)\n",
    "ensemble_mae_arima_lstm_gru = mean_absolute_error(aligned_y_test, ensemble_arima_lstm_gru)\n",
    "ensemble_mae_arima_lstm_gru_rf = mean_absolute_error(aligned_y_test, ensemble_arima_lstm_gru_rf)   \n",
    "print(f\"Ensemble ARIMA + LSTM MAE: {ensemble_mae_arima_lstm:.6f}\")\n",
    "print(f\"Ensemble ARIMA + LSTM + GRU MAE: {ensemble_mae_arima_lstm_gru:.6f}\")\n",
    "print(f\"Ensemble ARIMA + LSTM + GRU + RF MAE: {ensemble_mae_arima_lstm_gru_rf:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a68759",
   "metadata": {},
   "source": [
    "# 5) Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "COMPANY = 'CBA.AX'\n",
    "DATA_DIR = \"data\"\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "# --- 1. Load the Trained Models ---\n",
    "print(\"--- Loading Trained Models ---\")\n",
    "try:\n",
    "    # Define model paths\n",
    "    lstm_model_path = os.path.join(MODEL_DIR, f\"{COMPANY}_lstm_model.keras\")\n",
    "    gru_model_path = os.path.join(MODEL_DIR, f\"{COMPANY}_gru_model.keras\")\n",
    "    rnn_model_path = os.path.join(MODEL_DIR, f\"{COMPANY}_rnn_model.keras\")\n",
    "\n",
    "    # Load the models\n",
    "    lstm_model = load_model(lstm_model_path)\n",
    "    gru_model = load_model(gru_model_path)\n",
    "    rnn_model = load_model(rnn_model_path)\n",
    "    \n",
    "    print(f\"✅ Successfully loaded 3 models: LSTM, GRU, and RNN.\")\n",
    "    models = {'LSTM': lstm_model, 'GRU': gru_model, 'RNN': rnn_model}\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading models: {e}\")\n",
    "    print(\"Please ensure that the training cells have been run and the .keras files are saved in the 'models' directory.\")\n",
    "    # Stop execution if models can't be loaded\n",
    "    raise\n",
    "\n",
    "# --- 2. Load the Scaler ---\n",
    "# The scaler is needed to inverse transform the predictions back to actual prices.\n",
    "print(\"\\n--- Loading Data Scaler ---\")\n",
    "try:\n",
    "    scalers_filename = f\"{DATA_DIR}/{COMPANY}_scalers.pkl\"\n",
    "    with open(scalers_filename, 'rb') as f:\n",
    "        scalers = pickle.load(f)\n",
    "    # We specifically need the scaler for the 'Close' price, which was our target variable.\n",
    "    close_scaler = scalers['Close']\n",
    "    print(\"✅ Scaler for 'Close' price loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading scalers: {e}\")\n",
    "    print(\"The 'scalers.pkl' file is required to understand the model's output.\")\n",
    "    # Stop execution if scaler isn't found\n",
    "    raise\n",
    "\n",
    "# --- 3. Make Predictions on the Test Set ---\n",
    "print(\"\\n--- Making Predictions on Test Data ---\")\n",
    "predictions = {}\n",
    "# Add a check for X_test existence\n",
    "if 'X_test' not in locals():\n",
    "    raise NameError(\"Variable 'X_test' not found. Please run the data preparation cell.\")\n",
    "    \n",
    "for name, model in models.items():\n",
    "    print(f\" > Predicting with {name} model...\")\n",
    "    predictions[name] = model.predict(X_test)\n",
    "\n",
    "print(\"✅ Predictions complete for all models.\")\n",
    "\n",
    "# --- 4. Inverse Transform Predictions and Actual Values ---\n",
    "# The model predicts scaled values. We need to revert them to their original scale (dollars).\n",
    "print(\"\\n--- De-scaling Predictions for Comparison ---\")\n",
    "\n",
    "# De-scale the actual test values (y_test) for comparison\n",
    "if 'y_test' not in locals():\n",
    "    raise NameError(\"Variable 'y_test' not found. Please run the data preparation cell.\")\n",
    "actual_prices = close_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# De-scale the predicted values from each model\n",
    "descaled_predictions = {}\n",
    "for name, pred in predictions.items():\n",
    "    descaled_predictions[name] = close_scaler.inverse_transform(pred)\n",
    "    print(f\"✅ De-scaled {name} predictions.\")\n",
    "\n",
    "# --- 5. Calculate and Display Overall Prediction Metrics ---\n",
    "print(\"\\n--- Overall Prediction Performance Metrics ---\")\n",
    "metrics = []\n",
    "for name, pred_prices in descaled_predictions.items():\n",
    "    mae = mean_absolute_error(actual_prices, pred_prices)\n",
    "    mse = mean_squared_error(actual_prices, pred_prices)\n",
    "    rmse = np.sqrt(mse)\n",
    "    metrics.append({'Model': name, 'MAE ($)': mae, 'MSE': mse, 'RMSE ($)': rmse})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.set_index('Model', inplace=True)\n",
    "\n",
    "print(metrics_df.round(4))\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Detailed Analysis for Each Model ---\n",
    "for name, pred_prices in descaled_predictions.items():\n",
    "    print(f\"\\n\\n--- Detailed Analysis for {name} Model ---\")\n",
    "    print(\"=================================================\")\n",
    "    \n",
    "    # Create a results DataFrame for the current model\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual_Price': actual_prices.flatten(),\n",
    "        'Predicted_Price': pred_prices.flatten()\n",
    "    })\n",
    "    results_df['Error'] = results_df['Actual_Price'] - results_df['Predicted_Price']\n",
    "    results_df['Absolute_Error'] = np.abs(results_df['Error'])\n",
    "    \n",
    "    # a. Save predictions to CSV\n",
    "    predictions_filename = os.path.join(DATA_DIR, f\"{COMPANY}_predictions_{name}.csv\")\n",
    "    results_df.to_csv(predictions_filename, index=False)\n",
    "    print(f\"✅ Predictions for {name} model saved to: {predictions_filename}\")\n",
    "\n",
    "    # b. Visualize predictions vs actual and c. Plot prediction errors\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "    fig.suptitle(f'{name} Model: Prediction Analysis', fontsize=16)\n",
    "\n",
    "    # Plotting actual vs predicted\n",
    "    ax1.plot(results_df.index, results_df['Actual_Price'], label='Actual Prices', color='blue', marker='.', linestyle='', alpha=0.7)\n",
    "    ax1.plot(results_df.index, results_df['Predicted_Price'], label='Predicted Prices', color='red', linestyle='-', alpha=0.8)\n",
    "    ax1.set_ylabel('Stock Price ($)')\n",
    "    ax1.set_title('Actual vs. Predicted Prices')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Plotting prediction errors\n",
    "    ax2.plot(results_df.index, results_df['Error'], label='Prediction Error (Actual - Predicted)', color='green', linestyle='-')\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax2.set_xlabel('Test Data Point Index')\n",
    "    ax2.set_ylabel('Prediction Error ($)')\n",
    "    ax2.set_title('Prediction Errors')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    # d. Additional analysis: Best and worst predictions\n",
    "    print(\"\\n--- Prediction Highlights & Lowlights ---\")\n",
    "    best_predictions = results_df.nsmallest(5, 'Absolute_Error')\n",
    "    worst_predictions = results_df.nlargest(5, 'Absolute_Error')\n",
    "    \n",
    "    print(\"\\n✅ 5 Best Predictions (Lowest Absolute Error):\")\n",
    "    print(best_predictions.round(2))\n",
    "    \n",
    "    print(\"\\n❌ 5 Worst Predictions (Highest Absolute Error):\")\n",
    "    print(worst_predictions.round(2))\n",
    "\n",
    "    # e. Statistical summary of errors\n",
    "    print(\"\\n--- Statistical Summary of Errors ---\")\n",
    "    print(results_df['Error'].describe().round(4))\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "COMPANY = 'CBA.AX'\n",
    "DATA_DIR = \"data\"\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "# --- 1. Load the Trained Models ---\n",
    "print(\"--- Loading Trained Models ---\")\n",
    "try:\n",
    "    # Define model paths\n",
    "    lstm_model_path = os.path.join(MODEL_DIR, f\"{COMPANY}_lstm_model.keras\")\n",
    "    gru_model_path = os.path.join(MODEL_DIR, f\"{COMPANY}_gru_model.keras\")\n",
    "    rnn_model_path = os.path.join(MODEL_DIR, f\"{COMPANY}_rnn_model.keras\")\n",
    "\n",
    "    # Load the models\n",
    "    lstm_model = load_model(lstm_model_path)\n",
    "    gru_model = load_model(gru_model_path)\n",
    "    rnn_model = load_model(rnn_model_path)\n",
    "    \n",
    "    print(f\"✅ Successfully loaded 3 models: LSTM, GRU, and RNN.\")\n",
    "    models = {'LSTM': lstm_model, 'GRU': gru_model, 'RNN': rnn_model}\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading models: {e}\")\n",
    "    print(\"Please ensure that the training cells have been run and the .keras files are saved in the 'models' directory.\")\n",
    "    # Stop execution if models can't be loaded\n",
    "    raise\n",
    "\n",
    "# --- 2. Load the Scaler ---\n",
    "# The scaler is needed to inverse transform the predictions back to actual prices.\n",
    "print(\"\\n--- Loading Data Scaler ---\")\n",
    "try:\n",
    "    scalers_filename = f\"{DATA_DIR}/{COMPANY}_scalers.pkl\"\n",
    "    with open(scalers_filename, 'rb') as f:\n",
    "        scalers = pickle.load(f)\n",
    "    # We specifically need the scaler for the 'Close' price, which was our target variable.\n",
    "    close_scaler = scalers['Close']\n",
    "    print(\"✅ Scaler for 'Close' price loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading scalers: {e}\")\n",
    "    print(\"The 'scalers.pkl' file is required to understand the model's output.\")\n",
    "    # Stop execution if scaler isn't found\n",
    "    raise\n",
    "\n",
    "# --- 3. Make Predictions on the Test Set ---\n",
    "print(\"\\n--- Making Predictions on Test Data ---\")\n",
    "predictions = {}\n",
    "# Add a check for X_test existence\n",
    "if 'X_test' not in locals():\n",
    "    # Attempt to recreate X_test if it doesn't exist (e.g., running script in a new session)\n",
    "    try:\n",
    "        print(\"Variable 'X_test' not found. Attempting to recreate from saved data...\")\n",
    "        # This assumes 'test_data' and 'PREDICTION_DAYS' are available from previous cells\n",
    "        test_values = scaled_test_data[['Close', 'High', 'Low', 'Volume']].values\n",
    "        X_test, y_test = create_sequences(test_values, PREDICTION_DAYS)\n",
    "        print(\"✅ 'X_test' and 'y_test' recreated.\")\n",
    "    except NameError:\n",
    "         raise NameError(\"Critical variables ('X_test', 'test_data', 'PREDICTION_DAYS') not found. Please run the data preparation cells.\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\" > Predicting with {name} model...\")\n",
    "    predictions[name] = model.predict(X_test)\n",
    "\n",
    "print(\"✅ Predictions complete for all models.\")\n",
    "\n",
    "# --- 4. Inverse Transform Predictions and Actual Values ---\n",
    "# The model predicts scaled values. We need to revert them to their original scale (dollars).\n",
    "print(\"\\n--- De-scaling Predictions for Comparison ---\")\n",
    "\n",
    "# De-scale the actual test values (y_test) for comparison\n",
    "if 'y_test' not in locals():\n",
    "    raise NameError(\"Variable 'y_test' not found. Please run the data preparation cell.\")\n",
    "actual_prices = close_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# De-scale the predicted values from each model\n",
    "descaled_predictions = {}\n",
    "for name, pred in predictions.items():\n",
    "    descaled_predictions[name] = close_scaler.inverse_transform(pred)\n",
    "    print(f\"✅ De-scaled {name} predictions.\")\n",
    "\n",
    "# --- 5. Calculate and Display Overall Prediction Metrics ---\n",
    "print(\"\\n--- Overall Prediction Performance Metrics ---\")\n",
    "metrics = []\n",
    "for name, pred_prices in descaled_predictions.items():\n",
    "    mae = mean_absolute_error(actual_prices, pred_prices)\n",
    "    mse = mean_squared_error(actual_prices, pred_prices)\n",
    "    rmse = np.sqrt(mse)\n",
    "    metrics.append({'Model': name, 'MAE ($)': mae, 'MSE': mse, 'RMSE ($)': rmse})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.set_index('Model', inplace=True)\n",
    "\n",
    "print(metrics_df.round(4))\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Detailed Analysis for Each Model ---\n",
    "for name, pred_prices in descaled_predictions.items():\n",
    "    print(f\"\\n\\n--- Detailed Analysis for {name} Model ---\")\n",
    "    print(\"=================================================\")\n",
    "    \n",
    "    # Create a results DataFrame for the current model\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual_Price': actual_prices.flatten(),\n",
    "        'Predicted_Price': pred_prices.flatten()\n",
    "    })\n",
    "    results_df['Error'] = results_df['Actual_Price'] - results_df['Predicted_Price']\n",
    "    results_df['Absolute_Error'] = np.abs(results_df['Error'])\n",
    "    \n",
    "    # a. Save predictions to CSV\n",
    "    predictions_filename = os.path.join(DATA_DIR, f\"{COMPANY}_predictions_{name}.csv\")\n",
    "    results_df.to_csv(predictions_filename, index=False)\n",
    "    print(f\"✅ Predictions for {name} model saved to: {predictions_filename}\")\n",
    "\n",
    "    # b. Visualize predictions vs actual and c. Plot prediction errors\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "    fig.suptitle(f'{name} Model: Prediction Analysis', fontsize=16)\n",
    "\n",
    "    # Plotting actual vs predicted\n",
    "    ax1.plot(results_df.index, results_df['Actual_Price'], label='Actual Prices', color='blue', marker='.', linestyle='', alpha=0.7)\n",
    "    ax1.plot(results_df.index, results_df['Predicted_Price'], label='Predicted Prices', color='red', linestyle='-', alpha=0.8)\n",
    "    ax1.set_ylabel('Stock Price ($)')\n",
    "    ax1.set_title('Actual vs. Predicted Prices')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Plotting prediction errors\n",
    "    ax2.plot(results_df.index, results_df['Error'], label='Prediction Error (Actual - Predicted)', color='green', linestyle='-')\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax2.set_xlabel('Test Data Point Index')\n",
    "    ax2.set_ylabel('Prediction Error ($)')\n",
    "    ax2.set_title('Prediction Errors')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    # d. Additional analysis: Best and worst predictions\n",
    "    print(\"\\n--- Prediction Highlights & Lowlights ---\")\n",
    "    best_predictions = results_df.nsmallest(5, 'Absolute_Error')\n",
    "    worst_predictions = results_df.nlargest(5, 'Absolute_Error')\n",
    "    \n",
    "    print(\"\\n✅ 5 Best Predictions (Lowest Absolute Error):\")\n",
    "    print(best_predictions.round(2))\n",
    "    \n",
    "    print(\"\\n❌ 5 Worst Predictions (Highest Absolute Error):\")\n",
    "    print(worst_predictions.round(2))\n",
    "\n",
    "    # e. Statistical summary of errors\n",
    "    print(\"\\n--- Statistical Summary of Errors ---\")\n",
    "    print(results_df['Error'].describe().round(4))\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to aggregate daily data into N-day candlesticks ---\n",
    "def create_candlestick_data(df, n_days=1):\n",
    "    \"\"\"\n",
    "    Aggregates a DataFrame of daily stock data into multi-day candlesticks.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing daily OHLCV and prediction data.\n",
    "                           Must include 'Date', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                           'Actual_Price', and 'Predicted_Price'.\n",
    "        n_days (int): The number of trading days to aggregate into one candlestick.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the aggregated candlestick data.\n",
    "    \"\"\"\n",
    "    # Ensure 'Date' is a column and in datetime format for proper grouping\n",
    "    if 'Date' not in df.columns:\n",
    "        df = df.reset_index()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    if n_days == 1:\n",
    "        return df\n",
    "\n",
    "    # Aggregate data into N-day groups\n",
    "    # The `//` operator performs integer division to create groups of `n_days` size\n",
    "    grouped = df.groupby(np.arange(len(df)) // n_days).agg(\n",
    "        Date=('Date', 'first'),          # Start date of the period\n",
    "        Open=('Open', 'first'),          # Opening price of the period\n",
    "        High=('High', 'max'),            # Max high during the period\n",
    "        Low=('Low', 'min'),              # Min low during the period\n",
    "        Close=('Close', 'last'),         # Closing price of the period\n",
    "        Volume=('Volume', 'sum'),        # Sum of volume over the period\n",
    "        # Use the last value for predictions as it represents the period's end\n",
    "        Actual_Price=('Actual_Price', 'last'),\n",
    "        Predicted_Price=('Predicted_Price', 'last'),\n",
    "    )\n",
    "    return grouped\n",
    "\n",
    "# --- Main plotting function ---\n",
    "def plot_prediction_candlesticks(df, title, n_days, limit_days):\n",
    "    \"\"\"\n",
    "    Creates a candlestick chart from stock data and overlays model predictions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data to plot.\n",
    "        title (str): The title for the chart.\n",
    "        n_days (int): The number of trading days per candlestick.\n",
    "        limit_days (int): The number of recent data points (candlesticks) to display.\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_data = df.copy()\n",
    "    if limit_days:\n",
    "        plot_data = plot_data.tail(limit_days)\n",
    "    \n",
    "    # Aggregate data into N-day candlesticks if needed\n",
    "    plot_data = create_candlestick_data(plot_data, n_days=n_days)\n",
    "    \n",
    "    if plot_data.empty:\n",
    "        print(\"No data available to plot for the given parameters.\")\n",
    "        return\n",
    "\n",
    "    # Create subplots for price and volume\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 12), sharex=True, \n",
    "                                   height_ratios=[3, 1], gridspec_kw={'hspace': 0.1})\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Convert dates to matplotlib's internal float format for plotting\n",
    "    plot_data['Date_Num'] = mdates.date2num(pd.to_datetime(plot_data['Date']))\n",
    "    \n",
    "    # Dynamically set width of candlesticks based on aggregation\n",
    "    width = 0.8 * n_days\n",
    "\n",
    "    # --- Plot Candlesticks and Predictions on the top subplot (ax1) ---\n",
    "    for index, row in plot_data.iterrows():\n",
    "        color = 'green' if row['Close'] >= row['Open'] else 'red'\n",
    "        # Plot the wicks (high-low lines)\n",
    "        ax1.plot([row['Date_Num'], row['Date_Num']], [row['Low'], row['High']], color='black', linewidth=1)\n",
    "        # Create the candlestick body\n",
    "        body = Rectangle((row['Date_Num'] - width/2, min(row['Open'], row['Close'])), width, abs(row['Close'] - row['Open']), facecolor=color, edgecolor='black')\n",
    "        ax1.add_patch(body)\n",
    "\n",
    "    # Overlay Actual and Predicted Prices\n",
    "    ax1.plot(plot_data['Date_Num'], plot_data['Actual_Price'], label='Actual Price (Close)', color='blue', linestyle='--', alpha=0.9, linewidth=2)\n",
    "    ax1.plot(plot_data['Date_Num'], plot_data['Predicted_Price'], label='Predicted Price', color='orange', linestyle='-', alpha=0.9, linewidth=2)\n",
    "    \n",
    "    # --- Plot Volume on the bottom subplot (ax2) ---\n",
    "    volume_colors = ['green' if row['Close'] >= row['Open'] else 'red' for index, row in plot_data.iterrows()]\n",
    "    ax2.bar(plot_data['Date_Num'], plot_data['Volume'], width=width, color=volume_colors, alpha=0.6)\n",
    "\n",
    "    # --- Formatting and Labels ---\n",
    "    ax1.set_ylabel('Price ($)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax2.set_ylabel('Volume')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Use matplotlib's date formatter for the x-axis\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    fig.autofmt_xdate() # Auto-rotate date labels for better visibility\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make room for suptitle\n",
    "    plt.show()\n",
    "\n",
    "# --- Interactive Loop to Select Model and Parameters ---\n",
    "while True:\n",
    "    print(\"\\n\\n--- Interactive Candlestick Visualization ---\")\n",
    "    print(\"Select a model to visualize:\")\n",
    "    print(\"1: LSTM\")\n",
    "    print(\"2: GRU\")\n",
    "    print(\"3: RNN\")\n",
    "    print(\"q: Quit\")\n",
    "\n",
    "    choice = input(\"Enter your choice (1/2/3/q): \").strip().lower()\n",
    "\n",
    "    if choice == 'q':\n",
    "        print(\"Exiting interactive visualization.\")\n",
    "        break\n",
    "    \n",
    "    if choice not in ['1', '2', '3']:\n",
    "        print(\"❌ Invalid choice. Please try again.\")\n",
    "        continue\n",
    "\n",
    "    model_map = {'1': 'LSTM', '2': 'GRU', '3': 'RNN'}\n",
    "    model_name = model_map[choice]\n",
    "\n",
    "    # Get user input for chart parameters\n",
    "    try:\n",
    "        n_days_input = input(\"Enter number of days per candlestick (e.g., 1 for daily, 5 for weekly) [default: 1]: \").strip()\n",
    "        n_days = int(n_days_input) if n_days_input else 1\n",
    "        \n",
    "        limit_days_input = input(\"Enter number of recent days to display (e.g., 90) [default: 90]: \").strip()\n",
    "        limit_days = int(limit_days_input) if limit_days_input else 90\n",
    "    except ValueError:\n",
    "        print(\"❌ Invalid number. Using defaults (1 day per candle, 90 days limit).\")\n",
    "        n_days = 1\n",
    "        limit_days = 90\n",
    "    \n",
    "    # --- Prepare the data for plotting ---\n",
    "    try:\n",
    "        # The predictions align with the test set, starting from PREDICTION_DAYS into the test_data\n",
    "        if 'test_data' not in locals():\n",
    "             raise NameError(\"'test_data' DataFrame not found. Please ensure the data splitting cell was run.\")\n",
    "        \n",
    "        # Check if we have enough data in test_data to offset\n",
    "        if len(test_data) <= PREDICTION_DAYS:\n",
    "            raise ValueError(f\"Not enough data in 'test_data' to create visualization. 'test_data' has {len(test_data)} rows, but requires at least {PREDICTION_DAYS + 1} for lookback.\")\n",
    "\n",
    "        # Create the dataframe for plotting by taking the relevant slice of the test data\n",
    "        # and adding the actual and predicted prices as new columns.\n",
    "        plot_df = test_data.iloc[PREDICTION_DAYS:].copy()\n",
    "        plot_df['Actual_Price'] = actual_prices.flatten()\n",
    "        plot_df['Predicted_Price'] = descaled_predictions[model_name].flatten()\n",
    "        \n",
    "        # --- FIX: Ensure 'Open' column exists for candlestick chart ---\n",
    "        if 'Open' not in plot_df.columns:\n",
    "            print(\"INFO: 'Open' column not found. Deriving it from previous day's 'Close'.\")\n",
    "            # Use previous day's close as the open price\n",
    "            plot_df['Open'] = plot_df['Close'].shift(1)\n",
    "            # For the first row, which will be NaN, use the 'Close' price of that day\n",
    "            plot_df['Open'].fillna(plot_df['Close'], inplace=True)\n",
    "\n",
    "        chart_title = f\"{COMPANY} - {model_name} Model Predictions vs. Actuals\"\n",
    "        \n",
    "        # Call the plotting function\n",
    "        plot_prediction_candlesticks(df=plot_df, title=chart_title, n_days=n_days, limit_days=limit_days)\n",
    "\n",
    "    except (NameError, ValueError) as e:\n",
    "        print(f\"❌ Error preparing data for plot: {e}\")\n",
    "        print(\"Please make sure the prerequisite cells for data preparation and model prediction have been run successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Prepare Data for Boxplots ---\n",
    "def create_boxplot_data(df, period='monthly'):\n",
    "    \"\"\"\n",
    "    Prepares and groups historical stock data for boxplot visualization.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with historical stock data.\n",
    "        period (str): The time period to group by ('monthly', 'quarterly', 'yearly', 'weekly').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an added 'Period' column for grouping.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Ensure 'Date' column is in datetime format\n",
    "    df_copy['Date'] = pd.to_datetime(df_copy['Date'])\n",
    "    \n",
    "    # Create a period label for grouping\n",
    "    if period == 'monthly':\n",
    "        df_copy['Period'] = df_copy['Date'].dt.strftime('%Y-%m')\n",
    "    elif period == 'quarterly':\n",
    "        df_copy['Period'] = df_copy['Date'].dt.to_period('Q').astype(str)\n",
    "    elif period == 'yearly':\n",
    "        df_copy['Period'] = df_copy['Date'].dt.year.astype(str)\n",
    "    elif period == 'weekly':\n",
    "        df_copy['Period'] = df_copy['Date'].dt.strftime('%Y-W%U')\n",
    "    else:\n",
    "        raise ValueError(\"Period must be one of 'monthly', 'quarterly', 'yearly', or 'weekly'\")\n",
    "    \n",
    "    # Calculate additional metrics for plotting\n",
    "    df_copy['Price_Range'] = df_copy['High'] - df_copy['Low']\n",
    "    df_copy['Daily_Return'] = df_copy['Close'].pct_change() * 100\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# --- Function to Plot Historical Price Boxplots ---\n",
    "def plot_price_boxplots(df, period='monthly'):\n",
    "    \"\"\"\n",
    "    Creates a 2x2 grid of boxplots for comprehensive historical stock price analysis.\n",
    "    \"\"\"\n",
    "    boxplot_data = create_boxplot_data(df, period)\n",
    "    \n",
    "    if boxplot_data.empty:\n",
    "        print(\"No data available for plotting.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    fig.suptitle(f'Historical Stock Analysis by {period.title()}', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Close Price Distribution\n",
    "    sns.boxplot(ax=axes[0, 0], data=boxplot_data, x='Period', y='Close', palette='viridis')\n",
    "    axes[0, 0].set_title('Close Price Distribution')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Daily Price Range (Volatility)\n",
    "    sns.boxplot(ax=axes[0, 1], data=boxplot_data, x='Period', y='Price_Range', palette='plasma')\n",
    "    axes[0, 1].set_title('Daily Price Range (Volatility)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Plot 3: Trading Volume\n",
    "    sns.boxplot(ax=axes[1, 0], data=boxplot_data, x='Period', y='Volume', palette='magma')\n",
    "    axes[1, 0].set_title('Trading Volume Distribution')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 4: Daily Returns\n",
    "    sns.boxplot(ax=axes[1, 1], data=boxplot_data, x='Period', y='Daily_Return', palette='cividis')\n",
    "    axes[1, 1].set_title('Daily Returns Distribution (%)')\n",
    "    axes[1, 1].axhline(0, color='black', linestyle='--')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# --- Function to Plot Prediction Error Boxplots ---\n",
    "def plot_prediction_error_boxplots(results_df, model_name):\n",
    "    \"\"\"\n",
    "    Creates boxplots for analyzing the errors of a prediction model.\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No prediction data available for plotting.\")\n",
    "        return\n",
    "        \n",
    "    results_df['Error'] = results_df['Actual_Price'] - results_df['Predicted_Price']\n",
    "    results_df['Absolute_Error'] = np.abs(results_df['Error'])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    fig.suptitle(f'{model_name} Model: Prediction Error Analysis', fontsize=16)\n",
    "\n",
    "    # Plot 1: Prediction Error\n",
    "    sns.boxplot(ax=ax1, y=results_df['Error'], palette='coolwarm')\n",
    "    ax1.axhline(0, color='black', linestyle='--')\n",
    "    ax1.set_title('Prediction Error (Actual - Predicted)')\n",
    "    ax1.set_ylabel('Error ($)')\n",
    "\n",
    "    # Plot 2: Absolute Prediction Error\n",
    "    sns.boxplot(ax=ax2, y=results_df['Absolute_Error'], palette='spring')\n",
    "    ax2.set_title('Absolute Prediction Error')\n",
    "    ax2.set_ylabel('Absolute Error ($)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# --- Interactive Loop for Boxplot Generation ---\n",
    "while True:\n",
    "    print(\"\\n\\n--- Boxplot Analysis Menu ---\")\n",
    "    print(\"Select the type of analysis:\")\n",
    "    print(\"1: Historical Data Analysis\")\n",
    "    print(\"2: Prediction Error Analysis\")\n",
    "    print(\"q: Quit\")\n",
    "    \n",
    "    main_choice = input(\"Enter your choice (1/2/q): \").strip().lower()\n",
    "\n",
    "    if main_choice == 'q':\n",
    "        print(\"Exiting boxplot analysis.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        if main_choice == '1':\n",
    "            # --- Historical Analysis ---\n",
    "            print(\"\\nSelect a period for historical analysis:\")\n",
    "            print(\"1: Monthly\")\n",
    "            print(\"2: Quarterly\")\n",
    "            print(\"3: Yearly\")\n",
    "            print(\"4: Weekly\")\n",
    "            period_choice = input(\"Enter your choice [default: 1]: \").strip()\n",
    "            \n",
    "            period_map = {'1': 'monthly', '2': 'quarterly', '3': 'yearly', '4': 'weekly'}\n",
    "            period = period_map.get(period_choice, 'monthly') # Default to monthly\n",
    "            \n",
    "            # Combine train and test data to get the full historical dataset\n",
    "            if 'train_data' not in locals() or 'test_data' not in locals():\n",
    "                raise NameError(\"'train_data' or 'test_data' not found.\")\n",
    "            \n",
    "            full_historical_df = pd.concat([train_data, test_data], ignore_index=True)\n",
    "            plot_price_boxplots(full_historical_df, period=period)\n",
    "\n",
    "        elif main_choice == '2':\n",
    "            # --- Prediction Analysis ---\n",
    "            print(\"\\nSelect a model to analyze:\")\n",
    "            print(\"1: LSTM\")\n",
    "            print(\"2: GRU\")\n",
    "            print(\"3: RNN\")\n",
    "            model_choice = input(\"Enter your choice [default: 1]: \").strip()\n",
    "\n",
    "            model_map = {'1': 'LSTM', '2': 'GRU', '3': 'RNN'}\n",
    "            model_name = model_map.get(model_choice, 'LSTM') # Default to LSTM\n",
    "\n",
    "            if 'actual_prices' not in locals() or 'descaled_predictions' not in locals():\n",
    "                raise NameError(\"Prediction data not found.\")\n",
    "            \n",
    "            prediction_df = pd.DataFrame({\n",
    "                'Actual_Price': actual_prices.flatten(),\n",
    "                'Predicted_Price': descaled_predictions[model_name].flatten()\n",
    "            })\n",
    "            plot_prediction_error_boxplots(prediction_df, model_name)\n",
    "\n",
    "        else:\n",
    "            print(\"❌ Invalid choice. Please try again.\")\n",
    "\n",
    "    except (NameError, KeyError, ValueError) as e:\n",
    "        print(f\"❌ An error occurred: {e}\")\n",
    "        print(\"Please make sure all prerequisite cells for data loading and model prediction have been run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340fb06",
   "metadata": {},
   "source": [
    "# 5) Advanced Future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_multistep_prediction(model, last_sequence, k, close_scaler):\n",
    "    \"\"\"\n",
    "    Solves the multistep prediction problem by predicting a sequence of closing prices.\n",
    "\n",
    "    This function makes a prediction for the next day, then uses that prediction\n",
    "    as an input to predict the day after, and so on, for 'k' days.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained neural network model.\n",
    "        last_sequence (np.ndarray): The last sequence of data from the dataset,\n",
    "                                    used as the initial input for the prediction.\n",
    "                                    Shape should be (1, timesteps, features).\n",
    "        k (int): The number of future days to predict.\n",
    "        close_scaler (sklearn.preprocessing.MinMaxScaler): The scaler object that\n",
    "                                                           was used to scale the\n",
    "                                                           closing price data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted closing prices for the next 'k' days.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    current_sequence = last_sequence.copy()\n",
    "\n",
    "    for _ in range(k):\n",
    "        # Predict the next time step\n",
    "        next_step_pred_scaled = model.predict(current_sequence)[0, 0]\n",
    "\n",
    "        # Append the prediction to the list\n",
    "        predictions.append(\n",
    "            close_scaler.inverse_transform([[next_step_pred_scaled]])[0, 0]\n",
    "        )\n",
    "\n",
    "        # Create a new input for the next prediction.\n",
    "        # We need to reshape the prediction to match the feature dimension of the input.\n",
    "        # For a univariate case, we can simply repeat the predicted value for all features.\n",
    "        num_features = current_sequence.shape[2]\n",
    "        new_step_features = np.repeat(next_step_pred_scaled, num_features).reshape(1, 1, num_features)\n",
    "\n",
    "        # Update the sequence by removing the oldest time step and adding the new one\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], new_step_features, axis=1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def solve_multivariate_prediction(model, multivariate_sequence, close_scaler):\n",
    "    \"\"\"\n",
    "    Solves the simple multivariate prediction problem for a single day.\n",
    "\n",
    "    This function takes a sequence of multivariate data and predicts the\n",
    "    closing price for the next day.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained neural network model.\n",
    "        multivariate_sequence (np.ndarray): A sequence of multivariate data with a\n",
    "                                            shape of (1, timesteps, features).\n",
    "        close_scaler (sklearn.preprocessing.MinMaxScaler): The scaler for the closing price.\n",
    "\n",
    "    Returns:\n",
    "        float: The predicted closing price for the next day.\n",
    "    \"\"\"\n",
    "    # Make a prediction using the multivariate sequence\n",
    "    prediction_scaled = model.predict(multivariate_sequence)[0, 0]\n",
    "\n",
    "    # Inverse transform the prediction to get the actual price\n",
    "    return close_scaler.inverse_transform([[prediction_scaled]])[0, 0]\n",
    "\n",
    "\n",
    "def solve_multivariate_multistep_prediction(model, last_sequence, k, scalers, feature_order):\n",
    "    \"\"\"\n",
    "    Solves the multivariate, multistep prediction problem.\n",
    "\n",
    "    This function predicts 'k' days into the future, using a multivariate input.\n",
    "    It assumes that future values for features other than the closing price are\n",
    "    the same as the last observed day.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained neural network model.\n",
    "        last_sequence (np.ndarray): The last available sequence of multivariate data.\n",
    "        k (int): The number of future days to predict.\n",
    "        scalers (dict): A dictionary of scaler objects for all features,\n",
    "                        e.g., {'Close': close_scaler, 'High': high_scaler, ...}.\n",
    "        feature_order (list): A list of feature names in the order they were used\n",
    "                              for training the model (e.g., ['Close', 'High', 'Low', 'Volume']).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predicted closing prices for the next 'k' days.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    current_sequence = last_sequence.copy()\n",
    "\n",
    "    for _ in range(k):\n",
    "        # Predict the closing price for the next day\n",
    "        next_close_scaled = model.predict(current_sequence)[0, 0]\n",
    "\n",
    "        # Inverse transform the predicted closing price and add it to our predictions list\n",
    "        predictions.append(\n",
    "            scalers['Close'].inverse_transform([[next_close_scaled]])[0, 0]\n",
    "        )\n",
    "\n",
    "        # Create the new time step for the next prediction\n",
    "        new_step = current_sequence[0, -1, :].copy()  # Start with the last known features\n",
    "\n",
    "        # Find the index of the 'Close' feature and update it with the new scaled prediction\n",
    "        close_feature_index = feature_order.index('Close')\n",
    "        new_step[close_feature_index] = next_close_scaled\n",
    "\n",
    "        # Reshape the new step to be appended to the sequence\n",
    "        new_step = new_step.reshape(1, 1, current_sequence.shape[2])\n",
    "\n",
    "        # Update the sequence by removing the oldest time step and adding the new one\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], new_step, axis=1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# --- Example Usage Cell ---\n",
    "# 1. Select the model you want to use (e.g., the GRU model)\n",
    "# Make sure the model variable is loaded from training or a file\n",
    "model_to_use = load_model('models/CBA.AX_gru_model.keras')\n",
    "\n",
    "# 2. Get the last sequence from the test data to start predicting from\n",
    "last_sequence = X_test[-1].reshape(1, PREDICTION_DAYS, X_test.shape[2])\n",
    "\n",
    "# 3. Specify how many days you want to predict into the future\n",
    "k_days_to_predict = 10\n",
    "\n",
    "# 4. Ensure your 'scalers' dictionary and 'feature_order' list are available\n",
    "# These should have been created during the data preprocessing steps\n",
    "# feature_order = ['Close', 'High', 'Low', 'Volume'] # Example, should match your training\n",
    "\n",
    "# 5. Call the function to get the future predictions\n",
    "future_predictions = solve_multivariate_multistep_prediction(\n",
    "    model_to_use,\n",
    "    last_sequence,\n",
    "    k_days_to_predict,\n",
    "    scalers,\n",
    "    ['Close', 'High', 'Low', 'Volume'] # Ensure this matches your model's training\n",
    ")\n",
    "\n",
    "# 6. Print the results\n",
    "print(f\"Predicted closing prices for the next {k_days_to_predict} days:\")\n",
    "for i, price in enumerate(future_predictions):\n",
    "    print(f\"Day +{i+1}: ${price:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
